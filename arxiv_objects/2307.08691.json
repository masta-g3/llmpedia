{"id": "http://arxiv.org/abs/2307.08691v1", "guidislink": true, "link": "http://arxiv.org/abs/2307.08691v1", "updated": "2023-07-17T17:50:36Z", "updated_parsed": [2023, 7, 17, 17, 50, 36, 0, 198, 0], "published": "2023-07-17T17:50:36Z", "published_parsed": [2023, 7, 17, 17, 50, 36, 0, 198, 0], "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work\n  Partitioning", "title_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=flashattention+2++faster+attention+with+better+parallelism+and+work+partitioning&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "FlashAttention-2: Faster Attention with Better Parallelism and Work\n  Partitioning"}, "summary": "Scaling Transformers to longer sequence lengths has been a major problem in\nthe last several years, promising to improve performance in language modeling\nand high-resolution image understanding, as well as to unlock new applications\nin code, audio, and video generation. The attention layer is the main\nbottleneck in scaling to longer sequences, as its runtime and memory increase\nquadratically in the sequence length. FlashAttention exploits the asymmetric\nGPU memory hierarchy to bring significant memory saving (linear instead of\nquadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines),\nwith no approximation. However, FlashAttention is still not nearly as fast as\noptimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the\ntheoretical maximum FLOPs/s. We observe that the inefficiency is due to\nsuboptimal work partitioning between different thread blocks and warps on the\nGPU, causing either low-occupancy or unnecessary shared memory reads/writes. We\npropose FlashAttention-2, with better work partitioning to address these\nissues. In particular, we (1) tweak the algorithm to reduce the number of\nnon-matmul FLOPs (2) parallelize the attention computation, even for a single\nhead, across different thread blocks to increase occupancy, and (3) within each\nthread block, distribute the work between warps to reduce communication through\nshared memory. These yield around 2$\\times$ speedup compared to FlashAttention,\nreaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close\nto the efficiency of GEMM operations. We empirically validate that when used\nend-to-end to train GPT-style models, FlashAttention-2 reaches training speed\nof up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization).", "summary_detail": {"type": "text/plain", "language": null, "base": "http://export.arxiv.org/api/query?search_query=flashattention+2++faster+attention+with+better+parallelism+and+work+partitioning&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=40", "value": "Scaling Transformers to longer sequence lengths has been a major problem in\nthe last several years, promising to improve performance in language modeling\nand high-resolution image understanding, as well as to unlock new applications\nin code, audio, and video generation. The attention layer is the main\nbottleneck in scaling to longer sequences, as its runtime and memory increase\nquadratically in the sequence length. FlashAttention exploits the asymmetric\nGPU memory hierarchy to bring significant memory saving (linear instead of\nquadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines),\nwith no approximation. However, FlashAttention is still not nearly as fast as\noptimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the\ntheoretical maximum FLOPs/s. We observe that the inefficiency is due to\nsuboptimal work partitioning between different thread blocks and warps on the\nGPU, causing either low-occupancy or unnecessary shared memory reads/writes. We\npropose FlashAttention-2, with better work partitioning to address these\nissues. In particular, we (1) tweak the algorithm to reduce the number of\nnon-matmul FLOPs (2) parallelize the attention computation, even for a single\nhead, across different thread blocks to increase occupancy, and (3) within each\nthread block, distribute the work between warps to reduce communication through\nshared memory. These yield around 2$\\times$ speedup compared to FlashAttention,\nreaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close\nto the efficiency of GEMM operations. We empirically validate that when used\nend-to-end to train GPT-style models, FlashAttention-2 reaches training speed\nof up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization)."}, "authors": [{"name": "Tri Dao"}], "author_detail": {"name": "Tri Dao"}, "author": "Tri Dao", "links": [{"href": "http://arxiv.org/abs/2307.08691v1", "rel": "alternate", "type": "text/html"}, {"title": "pdf", "href": "http://arxiv.org/pdf/2307.08691v1", "rel": "related", "type": "application/pdf"}], "arxiv_primary_category": {"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom"}, "tags": [{"term": "cs.LG", "scheme": "http://arxiv.org/schemas/atom", "label": null}]}